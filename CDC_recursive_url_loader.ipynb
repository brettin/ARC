{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0f3cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai langchain playwright beautifulsoup4 playwright install chroma chromadb tiktoken langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30768e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e34a7025-844b-410d-84dd-22bab2fc457f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Error code: 404 - {'error': {'message': 'The model `gpt-4-0314` has been deprecated, learn more here: https://platform.openai.com/docs/deprecations', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m cleaned_text \u001b[38;5;241m=\u001b[39m clean_text_with_gpt4(docs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcleaned_texts\u001b[49m:\n\u001b[1;32m     68\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Adding two newlines as a separator between texts\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaned texts saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaned_texts' is not defined"
     ]
    }
   ],
   "source": [
    "#Extract text from CDC website\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from openai import OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "import time\n",
    "\n",
    "#Send to GPT4 for cleanup\n",
    "def clean_text_with_gpt4(text):\n",
    "    \"\"\"\n",
    "    This function takes a string of text and uses GPT-4 to clean it up using the OpenAI ChatCompletion API.\n",
    "    It handles large texts by breaking them into smaller chunks.\n",
    "    :param text: String containing the text to be cleaned.\n",
    "    :return: Cleaned text as a string.\n",
    "    \"\"\"\n",
    "    cleaned_texts = []\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(text)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4-0314\",  # Assuming using the latest GPT-4 model\n",
    "                messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                          {\"role\": \"user\", \"content\": f\"Please clean up the following text:\\n\\n{chunk}\"}]\n",
    "            )\n",
    "            #print(response)\n",
    "            cleaned_texts.append(response.choices[0].message.content.strip())\n",
    "            #print(cleaned_texts)\n",
    "            time.sleep(1)  # Delay to respect rate limits\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "    return ' '.join(cleaned_texts)\n",
    "\n",
    "file_name = \"cleaned_texts.txt\"\n",
    "if os.path.exists(file_name):\n",
    "    print(f\"The file {file_name} already exists.\")\n",
    "    try:\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            #print(\"File content:\\n\")\n",
    "            #print(content)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {file_name} was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "else:\n",
    "    url = \"https://www.cdc.gov\"\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=url, max_depth=2, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    client = OpenAI()\n",
    "    cleaned_text = clean_text_with_gpt4(docs)\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        for text in cleaned_texts:\n",
    "            file.write(text + \"\\n\\n\")  # Adding two newlines as a separator between texts\n",
    "    print(f\"Cleaned texts saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd8749-3c13-4979-bf3a-ba71971323ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "text_splitter2 = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter2.split_documents(cleaned_text)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "CDC_retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73af828-adc6-4250-95ef-303534a6f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "file_name = \"EpiHiper-Schema-master.zip\"\n",
    "extracted_folder = \"extracted_files\"\n",
    "\n",
    "if os.path.exists(file_name):\n",
    "    print(f\"The file {file_name} already exists.\")\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extracted_folder)\n",
    "\n",
    "        docs = []\n",
    "\n",
    "        # Walk through the directory, including subdirectories\n",
    "        for foldername, subfolders, filenames in os.walk(extracted_folder):\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(foldername, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        docs.append(file.read())\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while reading {filename}: {e}\")\n",
    "\n",
    "        # At this point, 'docs' contains the contents of all files from the zip\n",
    "        # You can process 'docs' as needed\n",
    "\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"The file is not a zip file or it is corrupted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "code_splits = text_splitter2.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=code_splits, embedding=OpenAIEmbeddings())\n",
    "ABM_retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf065cfd-7486-43dd-8f3a-c7ac2733b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=.2)\n",
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0.0) #\"gpt-4-1106-preview\", \"gpt-4-0314\"\n",
    "\n",
    "query_planner = Agent(\n",
    "    role=\"Simulation Planner\",\n",
    "    goal=\"Plan the steps needed to parameterize an agent-based simulation from existing knowledge\",\n",
    "    backstory=textwrap.dedent(\"\"\"\n",
    "        You are an expert at identifying modeling parameters from code base that implements \n",
    "        an agent-based model and listing the model choices, parameters, and json config files \n",
    "        whose values need to be determined. You will break down each model choice, parameter, \n",
    "        and json config file into sub-questions such that the answer to each sub-question will \n",
    "        inform the value to be used in the agent-based simulation.\n",
    "        Accept the user-question and determine if it requires sub-questions to either the\n",
    "        CDC website which provides an official source of recent infectious disease outbreaks\n",
    "        or Wikipedia for information about a geographical location, country, infectious agent\n",
    "        characteristics, transmission dynamics, infection states, or other epidemiological\n",
    "        modeling efforts.\n",
    "        Your final answer MUST be a description of sub-questions that explain the best model\n",
    "        choices, model parameters, and config files for an agent-based modeling code base.\n",
    "    \"\"\"),\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    tools=[],  ###\n",
    "    llm=llm,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d12a7-d564-42c6-91a7-5f2cb284319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain import hub\n",
    "#prompt = hub.pull(\"hwchase17/self-ask-with-search\")\n",
    "#print(f'{prompt.format(agent_scratchpad = \"AGENTSCRATCHPAD\", input = \"INPUT\")}')\n",
    "\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "CDC_retriever_tool = create_retriever_tool(\n",
    "    CDC_retriever,\n",
    "    \"Intermediate Answer\",\n",
    "    \"\"\"As an AI assistant you provide answers based on the given context, ensuring accuracy and briefness. \n",
    "\n",
    "        You always follow these guidelines:\n",
    "\n",
    "        -If the answer isn't available within the context, state that fact\n",
    "        -Otherwise, answer to your best capability, refering to source of documents provided\n",
    "        -Only use examples if explicitly requested\n",
    "        -Do not introduce examples outside of the context\n",
    "        -Do not answer if context is absent\n",
    "        -Limit responses to three or four sentences for clarity and conciseness\n",
    "        \n",
    "        Search for data related to outbreaks. For questions about outbreaks, use this tool to return \n",
    "        relevant data for answering questions about outbreaks\"\"\",\n",
    ")\n",
    "\n",
    "ABM_retriever_tool = create_retriever_tool(\n",
    "    ABM_retriever,\n",
    "    \"Intermediate Answer\",\n",
    "    \"\"\"As an AI assistant you provide answers based on the given context, ensuring accuracy and briefness. \n",
    "\n",
    "        You always follow these guidelines:\n",
    "\n",
    "        -If the answer isn't available within the context, state that fact\n",
    "        -Otherwise, answer to your best capability, refering to source of documents provided\n",
    "        -Only use examples if explicitly requested\n",
    "        -Do not introduce examples outside of the context\n",
    "        -Do not answer if context is absent\n",
    "        -Limit responses to three or four sentences for clarity and conciseness\n",
    "        \n",
    "        Search for data related to outbreaks. For questions about outbreaks, use this tool to return \n",
    "        relevant data for answering questions about outbreaks\"\"\",\n",
    ")\n",
    "\n",
    "search = GoogleSearchAPIWrapper()\n",
    "Google_search_tool = Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events or specialized information about a specific domain\",\n",
    "    )\n",
    "\n",
    "query_executor = Agent(\n",
    "    role=\"\"\"\n",
    "        Searcher of information from CDC for current outbreak information, from the code base\n",
    "        for agent-based modeling simulation parameters and settings, or from Wikipedia for disease \n",
    "        specific information such a transmission, vectors, asymptomatic infectious carriers,\n",
    "        past modeling efforts, and so on.\n",
    "        \"\"\",\n",
    "    goal=\"Perform searches\",\n",
    "    backstory=textwrap.dedent(\"\"\"\n",
    "        Accept list of sub-questions from the query_planner agent and perform\n",
    "        the necessary searches to answer the questions.\n",
    "        Perform the tasks in the order given and report the result out.\n",
    "        Your final answer MUST be a correct response to the original user-query.\n",
    "    \"\"\"),\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    # tools=[SqlTools.do_sql_query, RagTools.do_rag_query],\n",
    "    tools=[CDC_retriever_tool, ABM_retriever_tool, Google_search_tool],\n",
    "    allow_delegation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dcba109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Find a list of gene products in genome 83332.12 which have the same EC number as the EC number for reaction rxn34000 ?\"\n",
    "\n",
    "task1 = Task(\n",
    "  description=textwrap.dedent(f\"\"\"\n",
    "      If any SQL access is required, you have access to a table that was created\n",
    "      with this SQL command:\n",
    "          ```sql\n",
    "              CREATE TABLE IF NOT EXISTS reactions (\n",
    "                  REACTION_ID TEXT PRIMARY KEY,\n",
    "                  EC TEXT\n",
    "          ```\n",
    "      Create a description of the sub-queries to handle this user-query:\n",
    "          {user_query}\n",
    "      Prepare the correct sub-queries (there may just be one) and pass them\n",
    "      along to the query_executor.  You should only use the SQL query capability\n",
    "      if the data is obviously in the SQL tables provided.\n",
    "  \"\"\"),\n",
    "  agent=query_planner\n",
    ")\n",
    "\n",
    "task2 = Task(\n",
    "    description=textwrap.dedent(f\"\"\"\n",
    "        You will receive a set of one or two sub-queries from the query_planner.\n",
    "        Use those tools to perform SQL queries and/or RAG (text) queries as necessary.\n",
    "        Your final answer MUST be a correct answer to the original user-query:\n",
    "            {user_query}\n",
    "    \"\"\"),\n",
    "    agent=query_executor\n",
    ")\n",
    "\n",
    "crew = Crew(\n",
    "    agents=[query_planner, query_executor],\n",
    "    tasks=[task1, task2],\n",
    "    verbose=2, # print what tasks are being worked on, can set it to 1 or 2\n",
    "    process=Process.sequential,\n",
    ")\n",
    "\n",
    "result = crew.kickoff()\n",
    "\n",
    "print(\"######################\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a565bf-bad9-41c0-b372-0d579a21fdaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
