{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/brettin/llm_tutorial/blob/main/tutorials/05-chains/05_langchain_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3fahVg9F5GMN",
    "outputId": "455e7917-89c5-4fc8-8095-fcc778b3aa90"
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain openai chromadb langchainhub bs4 tiktoken kaleido python-multipart cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "renzXk8Z5npr"
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3fjPVxE581L",
    "outputId": "3f0efe1c-4815-4530-cb5b-87ecd9f48af3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = getpass('Enter HUGGINGFACEHUB_API_TOKEN: ')\n",
    "os.environ['OPENAI_API_KEY'] = getpass(\"Enter OPENAI_API_KEY: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "es0FpcNt5t_x",
    "outputId": "3dcc723d-810b-401b-bea4-68e5e2f26e8c"
   },
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# print(prompt)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wafZi4IDxkXe",
    "outputId": "d6f8a86d-0b37-4f8d-b9c0-bb30112cf204"
   },
   "outputs": [],
   "source": [
    "!pip install arxiv\n",
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YlXvJFhZioqn"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RecursiveCharacterTextSplitter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# cleanup previous\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# vectorstore.delete_collection()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m docs \u001b[38;5;241m=\u001b[39m ArxivLoader(query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAntibiotic design using deep learning\u001b[39m\u001b[38;5;124m\"\u001b[39m, load_max_docs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m----> 9\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m \u001b[43mRecursiveCharacterTextSplitter\u001b[49m(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m     10\u001b[0m splits \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(docs)\n\u001b[1;32m     12\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(documents\u001b[38;5;241m=\u001b[39msplits, embedding\u001b[38;5;241m=\u001b[39mOpenAIEmbeddings())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RecursiveCharacterTextSplitter' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "from langchain.retrievers import ArxivRetriever\n",
    "\n",
    "# cleanup previous\n",
    "# vectorstore.delete_collection()\n",
    "\n",
    "docs = ArxivLoader(query=\"Antibiotic design using deep learning\", load_max_docs=20).load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "resp = rag_chain.invoke(\n",
    "  \"\"\"\n",
    "  Can you provide a summary of the current state of applying\n",
    "  deep learning to the discovery of new antibiotics?\n",
    "  \"\"\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 135
    },
    "id": "2WokPGM7dzdF",
    "outputId": "08aaaa0b-1bc2-4db1-9b1a-6c62c4990420"
   },
   "outputs": [],
   "source": [
    "resp.format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8bsLyMFxDk8"
   },
   "outputs": [],
   "source": [
    "vectorstore.delete_collection()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP+1qfMjP6dZ3MFqWpCEc7g",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
